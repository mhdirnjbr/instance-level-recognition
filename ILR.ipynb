{"cells":[{"cell_type":"markdown","metadata":{"id":"VMXQmPYx5zsL"},"source":["<h1><big><center>Object recognition and computer vision 2023/2024</center></big></h1>\n","\n","<h2><big><center> Assignment 1: Instance-level recognition</center></big></h2>\n","\n","<h5><big><center>Adapted from practicals from <a href=\"http://www.robots.ox.ac.uk/~vgg/practicals/overview/index.html\">Andrea Vedaldi and Andrew Zisserman</a>\n"," <br>by <a href=\"https://www.di.ens.fr/~varol/\">Gul Varol</a>, and <a href=\"https://www.di.ens.fr/~iroccosp/\">Ignacio Rocco</a>,\n"," <br>and from practicals from <a href=\"https://cs.nyu.edu/~fouhey/teaching/EECS442_W23\">David Fouhey</a> by <a href=\"https://16lemoing.github.io\">Guillaume Le Moing</a> </center></big></h5>\n","\n","</br>\n","<p align=\"center\">\n","<img height=300px src=\"http://www.di.ens.fr/willow/teaching/recvis17orig/assignment1/images/image12.png\"/></p>\n","<p align=\"center\"></p>"]},{"cell_type":"markdown","metadata":{"id":"JQ3NoPjbP6ur"},"source":["**STUDENT**:  YOUR NAME HERE\n","\n","**EMAIL**:  YOUR EMAIL HERE"]},{"cell_type":"markdown","metadata":{"id":"u8El-aD4x1hl"},"source":["# Guidelines\n","\n","The purpose of this assignment is that you get hands-on experience with the topics covered in class, which will help you understand these topics better. Therefore, **it is imperative that you do this assignment yourself. No code sharing will be tolerated.**\n","\n","Once you have completed the assignment, you will submit the `ipynb` file containing **both** code and results. For this, make sure to **run your notebook completely before submitting**.\n","\n","The `ipynb` must be named using the following format: **A1_LASTNAME_Firstname.ipynb**, and submitted in the **Google Classroom page**."]},{"cell_type":"markdown","metadata":{"id":"w2wRNvWF6Qu2"},"source":["# Goal\n","The goal of instance-level recognition is to match (recognize) a specific object or scene.  Examples include recognizing a specific building, such as Notre Dame, or a specific painting, such as `Starry Nightâ€™ by Van Gogh. The object is recognized despite changes in scale, camera viewpoint, illumination conditions and partial occlusion. An important application is image retrieval - starting from an image of an object of interest (the query), search  through an image dataset to obtain (or retrieve) those images that contain the target object.\n","\n","The goal of this assignment is to experiment and get basic practical experience with the methods that enable specific object recognition. It includes: (i) using SIFT features to obtain sparse matches between two images; (ii) using similarity co-variant detectors to cover changes in viewpoint; (iii) vector quantizing the SIFT descriptors into visual words to enable large scale retrieval; and (iv) constructing and using an image retrieval system to identify objects."]},{"cell_type":"markdown","metadata":{"id":"bpW3I6a66Sji"},"source":["# Setup environment\n","\n","We recommend using Google Colab (no GPU required)."]},{"cell_type":"markdown","metadata":{"id":"yTLfSBZe9QQE"},"source":["## Download and install CyVLFeat"]},{"cell_type":"markdown","source":["The new version of Cython in Colab is not compatible with the cyvlfeat libray so we need to downgrade it."],"metadata":{"id":"pptny0IeZEYl"}},{"cell_type":"code","source":["!pip install Cython==3.0.0a9"],"metadata":{"id":"5kkakGfrX0MQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Now we can install cyvlfeat."],"metadata":{"id":"CYpaegW2vRec"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"fCDiN_JPo1vD"},"outputs":[],"source":["!wget -N http://www.di.ens.fr/willow/teaching/recvis_orig/assignment1/install_cyvlfeat.py\n","%run install_cyvlfeat.py"]},{"cell_type":"markdown","metadata":{"id":"kEcxIyN3O_ME"},"source":["## Imports"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GFGh7fvKZyKT"},"outputs":[],"source":["# nice plot display\n","# uncomment if you are using jupyter lab, requires ipympl: https://github.com/matplotlib/jupyter-matplotlib\n","#%matplotlib widget\n","# uncomment if you are using jupyter notebook\n","# %matplotlib notebook\n","\n","import cyvlfeat\n","import numpy as np\n","from skimage.io import imread\n","from skimage.transform import resize, rotate\n","import matplotlib as mpl\n","import matplotlib.pyplot as plt\n","import warnings\n","from cyvlfeat.plot import plotframes\n","from scipy.io import loadmat\n","import numpy as np\n","\n","# change some default matplotlib parameters\n","mpl.rcParams['axes.grid'] = False\n","mpl.rcParams['figure.dpi'] = 120\n","\n","# ignore warnings\n","warnings.filterwarnings('ignore')"]},{"cell_type":"markdown","metadata":{"id":"wzE1SHZwPCF1"},"source":["## Download and uncompress data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lc-NOqeeAAzN"},"outputs":[],"source":["print('downloading assignment images...')\n","!wget -c http://www.di.ens.fr/willow/teaching/recvis_orig/assignment1/A1_images.tar.gz\n","print('done!')\n","print('uncompressing...')\n","!tar -xzf A1_images.tar.gz\n","print('done!')"]},{"cell_type":"markdown","metadata":{"id":"SD0WPt9P7kor"},"source":["# Part 1: Sparse features for matching specific objects in images"]},{"cell_type":"markdown","metadata":{"id":"Hvi7css7PFQW"},"source":["## Feature point detection\n","\n","The SIFT feature has both a *detector* and a *descriptor*. The *detector* used in SIFT corresponds to the \"difference of Gaussians\" (DoG) detector, which is an approximation of the \"Laplacian of Gaussian\"  (LoG) detector.\n","\n","We will start by computing and visualizing the SIFT feature detections (usually called frames) for two images of the same object (a building facade). Load an image, rotate and scale it, and then display the original and transformed pair:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"U1odHjjFDpjf"},"outputs":[],"source":["# Load an image\n","im1 = imread('data/oxbuild_lite/all_souls_000002.jpg')\n","\n","# Let the second image be a rotated and scaled version of the first\n","im1prime = rotate(np.pad(im1, ((0,0), (200,200),(0,0)), 'constant'), 35, 'bilinear')\n","h, w, _ = im1prime.shape\n","im1prime = resize(im1prime, (int(0.7 * h), int(0.7 * w)))\n","im1prime = (255 * im1prime).astype(np.uint8)\n","\n","f, (ax1, ax2) = plt.subplots(1, 2)\n","ax1.imshow(im1)\n","ax2.imshow(im1prime)\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"kLHQhVWDGpeT"},"source":["A SIFT frame is a circle with an orientation and is specified by four parameters: the center $x$, $y$, the scale $s$, and the rotation $\\theta$ (in radians), resulting in a vector of four parameters $(x, y, s, \\theta)$.\n","\n","Now compute and visualise the SIFT feature detections (frames):"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yj1q_hagYQmp"},"outputs":[],"source":["def rgb2gray(rgb):\n","    return np.float32(np.dot(rgb[..., :3], [0.299, 0.587, 0.114]) / 255.0)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"L6NN3Irxh37x"},"outputs":[],"source":["# Compute SIFT features for each\n","\n","[frames1, descrs1] = cyvlfeat.sift.sift(rgb2gray(im1), peak_thresh=0.01)\n","\n","[frames1prime, descrs1prime] = cyvlfeat.sift.sift(rgb2gray(im1prime), peak_thresh=0.01)\n","\n","f, (ax1, ax2) = plt.subplots(1, 2)\n","plt.sca(ax1)\n","plt.imshow(im1)\n","plotframes(frames1, linewidth=1)\n","\n","plt.sca(ax2)\n","plt.imshow(im1prime)\n","plotframes(frames1prime, linewidth=1)\n","\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"J7EE_bxoKlPG"},"source":["Examine the second image and its rotated and scaled version and convince yourself that the detections overlap the same scene regions (even though the circles' positions have moved in the image and their radius' have changed).\n","This demonstrates that the detection process (is co-variant or equi-variant) with translations, rotations and isotropic scalings. This class of transformations is known as a similarity or equiform."]},{"cell_type":"markdown","metadata":{"id":"wq0Z8XLZLQTk"},"source":["### :: TASK 1.1 ::\n","\n","Now repeat the exercise with a pair of natural images.\n","\n","Start by loading the second one: `data/oxbuild_lite/all_souls_000015.jpg`\n","\n","Plot the images and feature frames. Again you should see that many of the detections overlap the same scene region. Note that, while repeatability occurs for the pair of natural views, it is much better for the synthetically rotated pair."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"f5ZSA0olLwHK"},"outputs":[],"source":["##########################\n","#                        #\n","#  WRITE YOUR CODE HERE  #\n","#                        #\n","##########################"]},{"cell_type":"markdown","metadata":{"id":"TxipHsrTMdxC"},"source":["The number of detected features can be controlled by changing the peakThreshold option. A larger value will select features that correspond to higher contrast structures in the image."]},{"cell_type":"markdown","metadata":{"id":"auefGnqTMfkj"},"source":["### :: TASK 1.2 ::\n","\n","***For the same image, produce 3 sub-figures with different values of peakThreshold. Comment.***"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HKYdfbM-Mfvq"},"outputs":[],"source":["##########################\n","#                        #\n","#  WRITE YOUR CODE HERE  #\n","#                        #\n","##########################"]},{"cell_type":"markdown","metadata":{"id":"L9wplySFLtmY"},"source":["######################\n","\n","WRITE YOUR ANSWER HERE\n","\n","######################"]},{"cell_type":"markdown","metadata":{"id":"LpSJygkrMEOc"},"source":["## Feature point description and matching"]},{"cell_type":"markdown","metadata":{"id":"CQz1DjNYWErP"},"source":["### Introduction to feature descriptors\n","\n","The parameters $(t_x, t_y, s, \\theta)$ of the detected *frames*,  can be used to extract a **scaled and oriented RGB patch** around $(t_x,t_y)$, used to *describe* the feature point.\n","\n","The simplest possible descriptor would be to (i) resize these patches to a common size (eg. 30x30) and (ii) flatten to a vector. However, in practice we use more sophisticated descriptors such as SIFT, that is based on histograms of gradient orientations."]},{"cell_type":"markdown","metadata":{"id":"P1bbB04iSSVe"},"source":["### :: TASK 1.3 ::\n","\n","\n","***What is the interest of using SIFT descriptors over these flattened RGB patches?***"]},{"cell_type":"markdown","metadata":{"id":"jPAkIV3TSZun"},"source":["######################\n","\n","   WRITE YOUR ANSWER HERE  \n","\n","######################\n"]},{"cell_type":"markdown","metadata":{"id":"156S_udKTCm-"},"source":["### Descriptor matching\n","SIFT descriptors are 128-dimensional vectors, and can be directly *matched* to find correspondences between images. We will start with the simplest matching scheme (first nearest neighbour of descriptors in terms of Euclidean distance) and then add more sophisticated methods to eliminate any mismatches."]},{"cell_type":"markdown","metadata":{"id":"5DwgvwPXTzao"},"source":["### :: TASK 1.4 ::\n","\n","\n","For each descriptor in im1, assign a matching descriptor in im2 by picking its first nearest neighbour.\n","\n","Populate the second column of the matches vector."]},{"cell_type":"code","source":["def matching(frames1, frames2, descrs1, descrs2):\n","\n","    # number of detections in image 1\n","    N_frames1 = frames1.shape[0]\n","    # allocate matrix for storing the matches\n","    matches = np.zeros((N_frames1, 2), dtype=np.int)\n","    # the first column of the matrix are the indices on image 1: 0,1,2,....,N_frames1-1\n","    matches[:, 0] = range(N_frames1)\n","\n","    # write code to find the matches in image 2 of each feature in image 1\n","    # populate the second column of the matches vector\n","\n","    ##########################\n","    #                        #\n","    #  WRITE YOUR CODE HERE  #\n","    #                        #\n","    ##########################\n","\n","    return matches"],"metadata":{"id":"hwvQzZqKsZMN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Compute the matches."],"metadata":{"id":"rPZ_npCRwoIi"}},{"cell_type":"code","source":["matches = matching(frames1, frames2, descrs1, descrs2)"],"metadata":{"id":"ue_L1VotsY8T"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Plot the matches."],"metadata":{"id":"5brKbhA4wrDi"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"7hWjq9G2aHdh"},"outputs":[],"source":["def plot_matches(im1, im2, frames1, frames2, matches):\n","  plt.figure()\n","  plt.imshow(np.concatenate((im1,im2),axis=1))\n","  for idx in range(matches.shape[0]):\n","      i=matches[idx,0]\n","      j=matches[idx,1]\n","      # plot dots at feature positions\n","      plt.gca().scatter([frames1[i,0],im1.shape[1]+frames2[j,0]], [frames1[i,1],frames2[j,1]], s=5, c='green')\n","      # plot lines\n","      plt.plot([frames1[i,0],im1.shape[1]+frames2[j,0]],[frames1[i,1],frames2[j,1]],linewidth=0.5)\n","  plt.show()\n","\n","plot_matches(im1, im2, frames1, frames2, matches)"]},{"cell_type":"markdown","metadata":{"id":"VdnoQ3gcV9mK"},"source":["## Improving SIFT matching (i) using Loweâ€™s second nearest neighbour test\n"]},{"cell_type":"markdown","metadata":{"id":"84MPzJZpWkVO"},"source":["Lowe introduced a second nearest neighbour (2nd NN) test to identify, and hence remove, ambiguous matches. The idea is to identify distinctive matches by a threshold on the ratio of first to second NN distances.\n","\n","The ratio is:  $$NN_{ratio} = \\frac{1^{st}\\text{NN distance}}{2^{nd}\\text{NN distance}}.$$\n","\n"]},{"cell_type":"markdown","metadata":{"id":"IoSIB18cV0cX"},"source":["### :: TASK 1.5 ::\n","\n","***For each descriptor in im1, compute the ratio between the first and second nearest neighbour distances.***\n","\n","***Populate the ratio vector.***\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BOo3--vjXiiY"},"outputs":[],"source":["# allocate matrix for storing the matches\n","ratio = np.zeros((frames1.shape[0], 1))\n","\n","# write code to find the 1st/2nd NN ratio for each descriptor in im1\n","# populate the ratio vector\n","\n","##########################\n","#                        #\n","#  WRITE YOUR CODE HERE  #\n","#                        #\n","##########################"]},{"cell_type":"markdown","metadata":{"id":"f3gKZDeIYE_X"},"source":["The ratio vector will be now used to retain only the matches that are above a given threshold.\n","\n","A value of $NN_{threshold} = 0.8$ is often a good compromise between losing too many matches and rejecting mismatches."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"I9_OvKEcZEGG"},"outputs":[],"source":["NN_threshold = 0.8\n","\n","filtered_indices = np.flatnonzero(ratio < NN_threshold)\n","filtered_matches = matches[filtered_indices, :]"]},{"cell_type":"markdown","source":["Plot the filtered matches."],"metadata":{"id":"A6sUDIzXxAiz"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"paGZYlI7YDXn"},"outputs":[],"source":["plot_matches(im1, im2, frames1, frames2, filtered_matches)"]},{"cell_type":"markdown","metadata":{"id":"ITytjPQ6FhHZ"},"source":["## Improving SIFT matching (ii) by geometric verification"]},{"cell_type":"markdown","metadata":{"id":"PsEEzjgVFshn"},"source":["In addition to the 2nd NN test, we can also require consistency between the matches and a geometric transformation between the images. For the moment we will look for matches that are consistent with a similarity transformation:\n","\n","$$\\begin{bmatrix} x_2 \\\\ y_2 \\end{bmatrix} =\n","sR(\\theta) \\begin{bmatrix} x_1 \\\\ y_1 \\end{bmatrix} + \\begin{bmatrix} t_x \\\\ t_y \\end{bmatrix} $$\n","\n","which consists of a rotation by $\\theta$, an isotropic scaling (i.e. same in all directions) by s, and a translation by a vector $(t_x, t_y)$. This transformation is specified by four parameters $(s,\\theta,t_x,t_y)$ and can be computed from a single correspondence between SIFT detections in each image."]},{"cell_type":"markdown","metadata":{"id":"mfk9XmtdGPZG"},"source":["### :: TASK 1.6 ::\n","\n","Given a detected feature with parameters $(x_1, y_1, s_1, \\theta_1)$ in image $1$ matching a feature $(x_2, y_2, s_2, \\theta_2)$ in image $2$, work out how to find out the parameters $(t_x,t_y,s,\\theta)$ of the transformation mapping points from image $1$ to image $2$.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"VF3nO5rX0IEX"},"source":["\n","$$\\theta=\\text{COMPLETE HERE}$$\n","$$s=\\text{COMPLETE HERE}$$\n","$$\\begin{bmatrix} t_x \\\\ t_y \\end{bmatrix} =\\text{COMPLETE HERE}$$\n"]},{"cell_type":"markdown","metadata":{"id":"56tVW8dt3t31"},"source":["The matches consistent with a similarity can then be found using the RANSAC algorithm, described by the following steps:\n","\n","For each tentative correspondence in turn:\n","\n","* compute the similarity transformation;\n","* map all the SIFT detections in one image to the other using this transformation;\n","* accept matches that are within a threshold distance to the mapped detection (inliers);\n","* count the number of accepted matches;\n","* optionally, fit a more accurate affine transformation or homography to the accepted matches and test re-validate the matches.\n","\n","Finally, choose the transformation with the highest count of inliers\n","\n","After this algorithm the inliers are consistent with the transformation and are retained, and most mismatches should now be removed."]},{"cell_type":"markdown","metadata":{"id":"_Bzv1Udh22Lm"},"source":["### :: TASK 1.7 ::"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vaoT7lbV2xVF"},"outputs":[],"source":["def ransac(frames1,frames2,matches,N_iters=100,dist_thresh=15):\n","    # initialize\n","    max_inliers = 0\n","    tnf = None\n","    # run random sampling\n","    for it in range(N_iters):\n","        # pick a random sample\n","        i = np.random.randint(0, frames1.shape[0])\n","        x_1, y_1, s_1, theta_1 = frames1[i, :]\n","        j = matches[i, 1]\n","        x_2, y_2, s_2, theta_2 = frames2[j, :]\n","\n","        # estimate transformation\n","\n","        # COMPLETE BELOW #\n","\n","        # theta =\n","        # s =\n","        # t_x =\n","        # t_y =\n","\n","        # evaluate estimated transformation\n","        X_1 = frames1[:, 0]\n","        Y_1 = frames1[:, 1]\n","        X_2 = frames2[matches[:, 1], 0]\n","        Y_2 = frames2[matches[:, 1], 1]\n","\n","        X_1_prime = s * (X_1 * np.cos(theta) - Y_1 * np.sin(theta)) + t_x\n","        Y_1_prime = s * (X_1 * np.sin(theta) + Y_1 * np.cos(theta)) + t_y\n","\n","        dist = np.sqrt((X_1_prime - X_2) ** 2 + (Y_1_prime - Y_2) ** 2)\n","\n","        inliers_indices = np.flatnonzero(dist < dist_thresh)\n","        num_of_inliers = len(inliers_indices)\n","\n","        # keep if best\n","        if num_of_inliers > max_inliers:\n","            max_inliers = num_of_inliers\n","            best_inliers_indices = inliers_indices\n","            tnf = [t_x, t_y, s, theta]\n","\n","    return (tnf, best_inliers_indices)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"M8ZpDU_1K5Md"},"outputs":[],"source":["tnf, inliers_indices = ransac(frames1, frames2, matches)\n","filtered_matches = matches[inliers_indices, :]"]},{"cell_type":"markdown","source":["Plot matches filtered with RANSAC."],"metadata":{"id":"SSbgEuCix3ro"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"99CjeXs6Ll1c"},"outputs":[],"source":["plot_matches(im1, im2, frames1, frames2, filtered_matches)"]},{"cell_type":"markdown","metadata":{"id":"z1bMD_ZkY4Zn"},"source":["## Augmented reality via template matching"]},{"cell_type":"markdown","metadata":{"id":"VbXcSW5yZsRT"},"source":["<center><img height=200px src=\"http://www.di.ens.fr/willow/teaching/recvis_orig/assignment1/augmented_reality.png\"/></p>\n","<p align=\"center\"></p></center>\n","\n","Now that we are able to draw consistent matches between pairs of images, we can use them to start replacing things in our reality. Imagine that we have a template\n","image and this template appears in the world but viewed at a different angle. We can fit a homography mapping between the template and the scene. Once we know the homography, we can use any new image to replace the template in the scene."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BfsVjkQ2ncl-"},"outputs":[],"source":["# Load the template and the scene\n","template = imread('data/augmented_reality/template.png')\n","scene = imread('data/augmented_reality/scene.png')\n","h, w, _ = scene.shape\n","factor = h / template.shape[0]\n","templatexl = (resize(template, (h, h)) * 255).astype(np.uint8)\n","\n","f, (ax1, ax2) = plt.subplots(1, 2)\n","\n","ax1.imshow(scene)\n","ax2.imshow(template)\n","ax1.title.set_text('Scene')\n","ax2.title.set_text('Template')\n","ax1.axis('off')\n","ax2.axis('off')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"isC5RvwirWgS"},"outputs":[],"source":["# Compute matches between the template and the scene\n","\n","[frames_scene, descrs_scene] = cyvlfeat.sift.sift(rgb2gray(scene), peak_thresh=0.01)\n","[frames_template, descrs_template] = cyvlfeat.sift.sift(rgb2gray(template), peak_thresh=0.01)\n","\n","matches = matching(frames_scene, frames_template, descrs_scene, descrs_template)\n","\n","plot_matches(scene, templatexl, frames_scene, factor * frames_template, matches)"]},{"cell_type":"code","source":["# Filter these matches with RANSAC\n","\n","tnf, inliers_indices = ransac(frames_scene, frames_template, matches)\n","filtered_matches = matches[inliers_indices, :]\n","\n","plot_matches(scene, templatexl, frames_scene, factor * frames_template, filtered_matches)"],"metadata":{"id":"fYlzjGe9mRqX"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We wish to fit a homography using the filtered matches $i$ in $1 .. N$, that is, to find the transformation $H$ which verifies for each point $(x_1^i, y_1^i)$ from the \"scene\" and its correspondence $(x_2^i, y_2^i)$ in the \"template\":\n","\n","$$ \\lambda \\mathbf{x_2^i} = H \\mathbf{x_1^i} $$\n","\n","with points expressed in homogenenous coordinates:\n","$$H=\\begin{bmatrix} h_{11} & h_{12} & h_{13} \\\\ h_{21} & h_{22} & h_{23} \\\\ h_{31} & h_{32} & h_{33} \\end{bmatrix}, \\,\\,\\,\n","\\mathbf{x_1^i}=\\begin{bmatrix} x_1^i \\\\ y_1^i \\\\ 1 \\end{bmatrix}, \\,\\,\\,\n","\\mathbf{x_2^i}=\\begin{bmatrix} x_2^i \\\\ y_2^i \\\\ 1 \\end{bmatrix}.\n","$$\n","\n","The scale ($\\lambda$) is arbitrary. We can also write this as:\n","$$\\mathbf{x_2^i}\\times (H \\mathbf{x_1^i}) = 0,$$\n","with $\\times$ the vectorial product."],"metadata":{"id":"QoVGe8NXvbBG"}},{"cell_type":"markdown","source":["### :: TASK 1.8 ::\n","\n","Given a match $(\\mathbf{x_1^i}, \\mathbf{x_2^i})$ and a flattened view of the matrix $H$:\n","$$\\mathbf{\\bar{h}}= \\begin{bmatrix} h_{11} & h_{12} & h_{13} & h_{21} & h_{22} & h_{23} & h_{31} & h_{32} & h_{33} \\end{bmatrix}^T,$$\n","\n","find a matrix $A_i$ such that the previous equation is expressed as a system\n","\n","$$A_i \\mathbf{\\bar{h}} = 0$$\n","\n","of linearly independent equations.\n","\n","$$A_i=\\text{COMPLETE HERE}$$"],"metadata":{"id":"3iPh6kCCwdw6"}},{"cell_type":"markdown","source":["### :: TASK 1.9 ::\n","\n","\n","***How many linearly independent equations each match gives us? How many degrees of freedom does $H$ have? Deduce the minimal number of matches to find a solution.***"],"metadata":{"id":"Cz60lc4muZT2"}},{"cell_type":"markdown","source":["######################\n","\n","   WRITE YOUR ANSWER HERE  \n","\n","######################"],"metadata":{"id":"KsxhywgF5JFB"}},{"cell_type":"markdown","source":["The complete system $A \\mathbf{\\bar{h}} = 0$ is built by concatenating the rows of $A_i$ for every match $i$ in $1 .. N$.\n","\n","A non-zero solution $\\mathbf{\\bar{h}}$ to this homogeneous least squares system is obtained by the singular value decomposition (SVD) of $A$. It is the singular vector corresponding to the smallest singular value. See for example [here](https://en.wikipedia.org/wiki/Singular_value_decomposition#Solving_homogeneous_linear_equations) if you are curious about the maths behind it."],"metadata":{"id":"BCgb8yaE5L-G"}},{"cell_type":"markdown","source":["### :: TASK 1.10 ::"],"metadata":{"id":"NLqWe3s-6O4e"}},{"cell_type":"code","source":["def compute_homography(frames1, frames2, matches):\n","    x_1 = frames1[matches[:,0],0]\n","    y_1 = frames1[matches[:,0],1]\n","    x_2 = frames2[matches[:,1],0]\n","    y_2 = frames2[matches[:,1],1]\n","\n","    # COMPLETE BELOW #\n","\n","    # A =\n","\n","    A = np.array(A)\n","\n","    # Solve for the homography using SVD\n","    _, _, Vt = np.linalg.svd(A)\n","    H = Vt[-1, :].reshape(3, 3)\n","\n","    # Normalize the homography matrix\n","    H /= H[2, 2]\n","\n","    return H"],"metadata":{"id":"Ly4X2Hfq6M69"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["H = compute_homography(frames_scene, frames_template, filtered_matches)\n","print(H)"],"metadata":{"id":"CU905NL7hUBH"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Now we will use the homography to replace the template in the scene by a new image."],"metadata":{"id":"jwsDpBjLQtYz"}},{"cell_type":"markdown","source":["### :: TASK 1.11 ::\n","\n","Complete the function below which takes as input the homography $H$, a template and the scene and which returns an image which is the result of warping the template into the scene and overlaying it on top of the original scene using alpha compositing.\n","\n","Use bilinear interpolation to determine pixel colors in the warped template given those in the original template.\n","\n","<center><img height=200px src=\"https://gabrielgambetta.com/computer-graphics-from-scratch/images/bilinear-texture-weights.png\"/></p>\n","<p align=\"center\"></p></center>\n","\n","More precisely, given a pixel location in the scene, let $C$ be its projection onto the template by applying the homography $H$. The resulting color of this pixel after the warping operation is a weighted sum of the color of the four nearest neighbours of $C$ in the template, with weights $(1-f_x) * (1-f_y)$, $f_x * (1-f_y)$, $(1-f_x) * f_y$ and $f_x*f_y$ for the top-left (TL), top-right (TR), bottom-left (BL), bottom-right (BR) pixels respectively.\n"],"metadata":{"id":"mAD1l0sJpu_4"}},{"cell_type":"code","source":["def apply_homography(H, template, scene):\n","    height, width, _ = scene.shape\n","\n","    # Initialize the warped template with the same dimensions as the scene\n","    w_template = np.zeros((height, width, 3), dtype=np.uint8)\n","\n","    # Initialize a mask indicating the pixels in the scene where the template is visible\n","    mask = np.zeros((height, width, 1), dtype=np.uint8)\n","\n","    # For each point in the scene\n","    for y in range(height):\n","        for x in range(width):\n","            # Apply the homography to find the corresponding point in the template (\"C\" in the illustration above)\n","            c = np.dot(H, [x, y, 1])\n","            c /= c[2]  # Normalize by the third homogeneous coordinate\n","            c = c[:2]  # Get cartesian coordinates from homogeneous coordinates\n","\n","            # Now check if the projected point (\"C\") is within the bounds of the template image\n","            # and update the mask (\"mask[y, x]\") accordingly.\n","\n","            # If it is inside the bounds, update the color of the warped template (\"w_template[y, x]\")\n","            # using colors from neighbouring pixels in the original \"template\" image (bilinear interpolation).\n","            # Note that \"w_template\" expects np.uint8 colors (discrete RGB values in 0..255)\n","\n","            ##########################\n","            #                        #\n","            #  WRITE YOUR CODE HERE  #\n","            #                        #\n","            ##########################\n","\n","    # Alpha compositing to overlay the template on top of the scene\n","    result = mask * w_template + (1 - mask) * scene\n","\n","    return result"],"metadata":{"id":"kg2NdyehhkUd"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Choose an image to replace the template in the scene."],"metadata":{"id":"10ocCB5WvKrq"}},{"cell_type":"code","source":["name = \"monk\"  # options [\"willow\", \"monk\", \"ens\"]"],"metadata":{"id":"kcZxZ-W0vEF1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Replace the template in the scene\n","\n","f, (ax1, ax2, ax3) = plt.subplots(1, 3)\n","replacement = imread(f'data/augmented_reality/{name}.png')\n","replacement = (resize(replacement, (template.shape[0], template.shape[1])) * 255).astype(np.uint8)\n","result = apply_homography(H, replacement, scene)\n","\n","ax1.imshow(template)\n","ax2.imshow(replacement)\n","ax3.imshow(result)\n","ax1.title.set_text('Template')\n","ax2.title.set_text('To Transfer')\n","ax3.title.set_text('Transferred')\n","ax1.axis('off')\n","ax2.axis('off')\n","ax3.axis('off')\n","plt.show()"],"metadata":{"id":"3_Vy5AoDnj1B"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vXz0ETuhP7vE"},"source":["# Part 2: Compact descriptors for image retrieval"]},{"cell_type":"markdown","metadata":{"id":"-7rqcpLhP_XJ"},"source":["In large scale retrieval the goal is to match a query image to a large database of images (for example the WWW or Wikipedia).\n","\n","The quality of an image match is measured as the number of geometrically verified feature correspondences between the query and a database image. While the techniques discussed in Part 1 are sufficient to do this, in practice they require too much memory to store the SIFT descriptors for all the detections in all the database images.\n","\n","In this part we will see how we can compute a *global* image descriptor from the set of SIFT descriptors using the bag-of-visual-words (BoVW) approach.\n","\n","Then, we will see how these global descriptors can be used to rapidly retrieve a shortlist of candidate database images given a query image. Finally, we will see how to re-rank the shortlist of candidates using a geometric verification technique that requires only the *detector frames* and their assigned visual word indices; remember the SIFT descriptors are only used to compute the compact BoVW descriptors and then discarded.\n"]},{"cell_type":"markdown","metadata":{"id":"mo0M1nsLMKQz"},"source":["## Download preprocessed dataset of paintings"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lfys5RNKa5aP"},"outputs":[],"source":["print('downloading dictionary of SIFT features and precomputed BoVW descriptors for painting dataset...')\n","!wget -c http://www.di.ens.fr/willow/teaching/recvis_orig/assignment1/paintings_imdb_SIFT_10k_preprocessed.mat\n","print('done!')"]},{"cell_type":"markdown","metadata":{"id":"ZgXUBMLkMYKR"},"source":["## Load preprocessed dataset of paintings"]},{"cell_type":"markdown","metadata":{"id":"0TtX0gzHMg6y"},"source":["We will now load the preprocessed dataset of paintings. The construction of this dataset has involved several steps.\n","\n","\n","1.   SIFT features were extracted from all painting in the dataset\n","2.   A global vocabulary of SIFT descriptors was computed using K-means clustering. These are the visual words of our dataset.\n","3.  The SIFT features for each painting were assigned to the nearest word, and a compact descriptor was generated for each painting. This compact descriptor consists in the normalized histogram of words. The histogram normalization itself involves 3 different steps:\n","    - a) TF-IDF weighting: each word is re-weighted according to its TF-IDF value. This removes weights to words that are very common and therefore not too descriptive.\n","    - b) Square-rooting: each element is square-rooted\n","    - c) L2-normalization: The whole histogram is L2-normalized.\n","    \n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vyQ8Ts0mUSEr"},"outputs":[],"source":["imdb=loadmat('paintings_imdb_SIFT_10k_preprocessed.mat')\n","\n","feature_vocab = np.transpose(imdb['vocab'])\n","imdb_hists = imdb['index']\n","imdb_tfidf = imdb['idf']\n","\n","imdb_url = lambda idx: imdb['images'][0][0][3][0][idx].item()\n","\n","num_words = feature_vocab.shape[0]\n","num_paintings = imdb_hists.shape[1]\n","\n","print('The vocabulary of SIFT features contains %s  visual words' % num_words)\n","print('The dictionary index contains %s histograms corresponding to each painting in the dataset' % num_paintings)\n","print('The tdf-idf vector has shape '+str(imdb_tfidf.shape))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hY3JQk0FB_gg"},"outputs":[],"source":["painting = imread('data/queries/mistery-painting1.jpg')\n","\n","[frames, descrs] = cyvlfeat.sift.sift(rgb2gray(painting), peak_thresh=0.01)\n","\n","plt.figure()\n","plt.imshow(painting)\n","plotframes(frames,linewidth=1)"]},{"cell_type":"markdown","metadata":{"id":"jHznDmPx8VzY"},"source":["### :: TASK 2.1 ::\n","\n","Construct a KDTree of the vocabulary for fast NN search. Then use the KDTree to find the closest word in the vocabulary to each descriptor of the query image.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Lg79y_1l8xIp"},"outputs":[],"source":["from sklearn.neighbors import KDTree\n","\n","##########################\n","#                        #\n","#  WRITE YOUR CODE HERE  #\n","#                        #\n","##########################"]},{"cell_type":"markdown","metadata":{"id":"RubSI5nU9C8j"},"source":["### :: TASK 2.2 ::\n","Compute the compact BoVW descriptor of the query image."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BOsAtuQYPS5R"},"outputs":[],"source":["query_hist = np.zeros((num_words, 1))\n","\n","##########################\n","#                        #\n","#  WRITE YOUR CODE HERE  #\n","#                        #\n","##########################"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FD-ZaEXGGXDu"},"outputs":[],"source":["# process histogram\n","query_hist = query_hist*imdb_tfidf\n","query_hist = np.sqrt(query_hist)\n","query_hist = query_hist/np.linalg.norm(query_hist)\n"]},{"cell_type":"markdown","metadata":{"id":"r3ylUZ_H9P0v"},"source":["### :: TASK 2.3 ::\n","\n","Compute the matching score with each image from the database.\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ls8i7Zd6-M7P"},"outputs":[],"source":["scores = np.zeros((1, num_paintings))\n","\n","##########################\n","#                        #\n","#  WRITE YOUR CODE HERE  #\n","#                        #\n","##########################"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tTbQ-MJjUvkH"},"outputs":[],"source":["# sort in descending order\n","scores_sorted_idx = np.argsort(-scores)\n","scores_sorted = scores.ravel()[scores_sorted_idx]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1EdES34d1Gxf"},"outputs":[],"source":["# plot top matches\n","N = 10\n","top_N_idx = scores_sorted_idx.ravel()[:N]\n","\n","plt.figure()\n","for i in range(N):\n","    # download images\n","    url = imdb_url(top_N_idx[i])\n","    img = imread(url, pilmode='RGB')\n","    # choose subplot\n","    plt.subplot(int(np.ceil(N/5)),5,i+1)\n","    # plot\n","    plt.imshow(img)\n","    plt.axis('off')\n","    plt.title('score %1.2f' % scores_sorted.ravel()[i])"]},{"cell_type":"markdown","metadata":{"id":"ReubKY5__-ds"},"source":["### :: TASK 2.4 (optional) ::\n","\n","Using the frame information and the assigned words of each descriptor, extract matching frames between the query and the shortlist of database images and perform geometric verification using a similarity transformation with no rotation.\n","\n","Compute the inlier scores and use it to re-rank this short list. Analyze the gap in the inlier count of the correct match and the incorrect ones."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oH5VEryQ-6__"},"outputs":[],"source":["imdb_frames = lambda idx: imdb['images'][0][0][5][0][idx] # usage: imdb_frames(idx)\n","imdb_words = lambda idx: imdb['images'][0][0][6][0][idx] # usage: imdb_words(idx)\n","\n","##########################\n","#                        #\n","#  WRITE YOUR CODE HERE  #\n","#                        #\n","##########################"]},{"cell_type":"markdown","metadata":{"id":"Im_0QKwzQZj5"},"source":["## AUTHORSHIP STATEMENT\n","\n","I declare that the preceding work was the sole result of my own effort and that I have not used any code or results from third-parties.\n","\n","FILL IN YOUR NAME HERE"]}],"metadata":{"colab":{"collapsed_sections":["CQz1DjNYWErP","156S_udKTCm-"],"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"}},"nbformat":4,"nbformat_minor":0}